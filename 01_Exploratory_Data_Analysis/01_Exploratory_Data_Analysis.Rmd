---
title: "Exploratory Data Analysis"
author: "Trevor Barnes"
date: "8/24/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(matrixStats)
library(corrplot)
library(ggplot2)
library(descr)
## Set the path for data loads
DATA_PATH <- file.path("~", "RStudio/practical_statistics_for_data_scientists/data")
## Import the state data
state <- read.csv(file.path(DATA_PATH, "state.csv"))
## Import the state dfw
dfw <- read.csv(file.path(DATA_PATH, "dfw_airline.csv"))
## Import the state sp500
sp500_px <- read.csv(file.path(DATA_PATH, "sp500_data.csv"))
## Import the state sp500 symbols
sp500_sym <- read.csv(file.path(DATA_PATH, "sp500_sectors.csv"))
## Import the King County Tax Data
kc_tax <- read.csv(file.path(DATA_PATH, "kc_tax.csv"))
## Import the Loan Data
lc_loans <- read.csv(file.path(DATA_PATH, "lc_loans.csv"))
## Import the Loan Data
airline_stats <- read.csv(file.path(DATA_PATH, "airline_stats.csv"))
```

***Exploratory data analysis***, or ***EDA***, is a comparatively new area of statistics. Classical statistics focused almost exclusively on *inference*, a sometimes complex set of procedures for drawing conclusions about large populations based on small samples. With the ready availability of computing power and expressive data analysis software, exploratory data analysis has evolved well beyond its original scope. Key drivers of this discipline have been the rapid development of new technology, access to more and bigger data, and the greater use of quantitative analysis in a variety of disciplines.

___

# Elements of Structured Data

Data comes from many sources: sensor measurements, events, text, images, and videos. The *Internet of Things (IoT)* is spewing out streams of information. Much of this data is *unstructured*: images are a collection of pixels with each pixel containing RGB (red, green, blue) color information. Texts are sequences of words and nonword characters, often organized by sections, subsections, and so on. *Clickstreams* are sequences of actions by a user interacting with an app or web page. In fact, a major challenge of data science is to harness this torrent of raw data into actionable information. 

| Term | Definition|Synonym|
|-------|-------|------|
| **Continuous** | Data that can take on any value in an interval. | interval, float, numeric|
| **Discrete** | Data that can take only integer values, such as counts. | integer, count | 
| **Categorical** | Data that can take on only a specific set of values representing a set of possible categories. | enums, enumerated, factors, nominal, polychotomous|
| **Binary** | A special case of categorical data with just two categories of values (0/1, true/false). | dichotomous, logical, indicator, boolean |
| **Ordinal** | Categorical data that has an explicit ordering. | ordered factor |
Table: **Key Terms for Data Types**

There are two basic types of structured data: ***numeric*** and ***categorical***. *Numeric* data comes in two forms: ***continuous***, such as wind speed or time duration, and ***discrete***, such as the count of the occurrence of an event. *Categorical* data takes only a fixed set of values, such as a type of TV screen (plasma, LCD, LED, etc.) or a state name (Alabama, Alaska, etc.). ***Binary*** data is an important special case of *categorical* data that takes on only one of two values, such as 0/1, yes/no, or true/false. Another useful type of categorical data is ***ordinal*** data in which the categories are ordered; an example of this is a numerical rating (1, 2, 3, 4, or 5).

Knowing that data is categorical can act as a signal telling software how statistical procedures, such as producing a chart or fitting a model, should behave. In particular, ordinal data can be represented as an `ordered.factor` in R and Python, preserving a user-specified ordering in charts, tables, and models. This also allows us to optimize storage and indexing in a relational database or in Spark. The possible values a given categorical variable can take are enforced in the software (like an enum).

|Key Ideas|
|-----|
|Data is typically classified in software by type.|
|Data types include continuous, discrete, categorical (which includes binary), and ordinal.|
|Data typing in software acts as a signal to the software on how to process the data.|

___
# Rectangular Data

The typical frame of reference for an analysis in data science is a ***rectangular data*** object, like a spreadsheet or database table.

| Term | Definition | Synonym |
|-----|-----|-----|
| **DataFrame** | Rectangular data (like a spreadsheet) is the basic data structure for statistical and machine learning models. | |
| **Feature** | A column in the table is commonly referred to as a feature.| attribute, input, predictor, variable | 
| **Outcome** | Many data science projects involve predicting an outcome—often a yes/no outcome. The features are sometimes used to predict the outcome in an experiment or study. | dependent variable, response, target, output |
| **Records** | A row in the table is commonly referred to as a record. | case, example, instance, observation, pattern, sample |

Rectangular data is essentially a two-dimensional matrix with rows indicating records (cases) and columns indicating features (variables). The data doesn’t always start in this form: unstructured data (e.g., text) must be processed and manipulated so that it can be represented as a set of features in the rectangular data. Data in relational databases must be extracted and put into a single table for most data analysis and modeling tasks.
Table: **Key Terms for Rectangular Data**

## Data Frames and Indexes

Traditional database tables have one or more columns designated as an index. This can vastly improve the efficiency of certain SQL queries. In Python, with the pandas library, the basic rectangular data structure is a DataFrame object. By default, an automatic integer index is created for a DataFrame based on the order of the rows. In pandas, it is also possible to set multilevel/hierarchical indexes to improve the efficiency of certain operations.

In R, the basic rectangular data structure is a data.frame object. A data.frame also has an implicit integer index based on the row order. While a custom key can be created through the row.names attribute, the native R data.frame does not support user-specified or multilevel indexes. To overcome this deficiency, two new packages are gaining widespread use: data.table and dplyr. Both support multilevel indexes and offer significant speedups in working with a data.frame.

<pre>
<center>TERMINOLOGY DIFFERENCES</center>
Terminology for rectangular data can be confusing. Statisticians and data scientists use different terms for the 
same thing. For a statistician, predictor variables are used in a model to predict a response or dependent 
variable. For a data scientist, features are used to predict a target. One synonym is particularly confusing: 
computer scientists will use the term sample for a single row; a sample to a statistician means a collection of 
rows.
</pre>

## Nonrectangular Data Structures

***Time series*** data records successive measurements of the same variable. It is the raw material for statistical forecasting methods, and it is also a key component of the data produced by devices—the *Internet of Things*.

***Spatial*** data structures, which are used in mapping and location analytics, are more complex and varied than rectangular data structures. In the ***object*** representation, the focus of the data is an object (e.g., a house) and its spatial coordinates. The ***field*** view, by contrast, focuses on small units of space and the value of a relevant metric (pixel brightness, for example).

***Graph*** (or ***Network***) data structures are used to represent physical, social, and abstract relationships. For example, a graph of a social network, such as Facebook or LinkedIn, may represent connections between people on the network. Distribution hubs connected by roads are an example of a physical network. Graph structures are useful for certain types of problems, such as network optimization and recommender systems.

| Key Ideas |
|-----|
| The basic data structure in data science is a rectangular matrix in which rows are records and columns are variables (features). |
| Terminology can be confusing; there are a variety of synonyms arising from the different disciplines that contribute to data science (statistics, computer science, and information technology). |

## Estimates of Location

Variables with measured or count data might have thousands of distinct values. A basic step in exploring your data is getting a “typical value” for each feature (variable): an estimate of where most of the data is located (i.e., its central tendency).

| Term | Definition | Synonym |
| ----- | ----- | -----|
| **Mean** | The sum of all values divided by the number of values. | average |
| **Weighted Mean** | The sum of all values times a weight divided by the sum of the weights. | weighted average |
| **Median** | The value such that one-half of the data lies above and below. | 50th percentile |
| **Weighted Mean** | The value such that one-half of the sum of the weights lies above and below the sorted data. ||
| **Trimmed Mean** | The average of all values after dropping a fixed number of extreme values. | truncated mean |
| **Robust** | Not sensitive to extreme values. | resistant |
| **Outlier** | A data value that is very different from most of the data. | extreme value |
Table: **Key Terms for Estimates of Location**

## Mean

The most basic estimate of location is the mean, or ***average value***. The mean is the sum of all the values divided by the number of values. Consider the following set of numbers: {3 5 1 2}. The mean is (3 + 5 + 1 + 2) / 4 = 11 / 4 = 2.75. You will encounter the symbol $Mean = \bar{x} = {\Sigma^n_ix_i\over{n}}$

> N (or n) refers to the total number of records or observations. In statistics it is capitalized if it is referring to a population, and lowercase if it refers to a sample from a population. In data science, that distinction is not vital so you may see it both ways.

A variation of the mean is a ***trimmed mean***, which you calculate by dropping a fixed number of sorted values at each end and then taking an average of the remaining values. Representing the sorted values by $x_{(1)}, x{(2)}, ..., x{(n)}$ where $x_{(1)}$ is the smallest value and $x_{(n)}$ the largest, the formula to compute the trimmed mean $P$ smallest and largest values omitted is:

$Trimmed Mean = \bar{x} = \frac{\Sigma^{n-p}_{i=p+1}x_{(i)}}{n-2p}$

A trimmed mean eliminates the influence of extreme values.

Another type of mean is a ***weighted mean***, which you calculate by multiplying each data value $x^i$ by a weight $w^i$ and dividing their sum by the sum of the weights. The formula for a weighted mean is:

$WeightedMean = \bar{x}_w = \frac{\Sigma^n_{i=1}w_ix_i}{\Sigma^n_iw_i}$

There are two main motivations for using a weighted mean:

1) Some values are intrinsically more variable than others, and highly variable observations are given a lower weight. For example, if we are taking the average from multiple sensors and one of the sensors is less accurate, then we might downweight the data from that sensor.
2) The data collected does not equally represent the different groups that we are interested in measuring. For example, because of the way an online experiment was conducted, we may not have a set of data that accurately reflects all groups in the user base. To correct that, we can give a higher weight to the values from the groups that were underrepresented.

## Median and Robust Estimates

The ***median*** is the middle number on a sorted list of the data. If there is an even number of data values, the middle value is one that is not actually in the data set, but rather the average of the two values that divide the sorted data into upper and lower halves. Compared to the mean, which uses all observations, the median depends only on the values in the center of the sorted data. While this might seem to be a disadvantage, since the mean is much more sensitive to the data, there are many instances in which the median is a better metric for location.

For the same reasons that one uses a weighted mean, it is also possible to compute a weighted median. As with the median, we first sort the data, although each data value has an associated weight.Instead of the middle number, the weighted median is a value such that the sum of the weights is equal for the lower and upper halves of the sorted list. Like the median, the weighted median is robust to outliers.

### Outliers

The median is referred to as a ***robust*** estimate of location since it is not influenced by ***outliers*** (extreme cases) that could skew the results. An outlier is any value that is very distant from the other values in a data set. The exact definition of an outlier is somewhat subjective. The median is not the only robust estimate of location. In fact, a trimmed mean is widely used to avoid the influence of outliers. 

Compute the mean, trimmed mean, and median for the population using R:

```{r}
## Get the mean for the state data
mean(state[["Population"]])

## Trim 10% from the data on each end
mean(state[["Population"]], trim=0.1)

## Get the median from the data
median(state[["Population"]])
```

The mean is bigger than the trimmed mean, which is bigger than the median.
This is because the trimmed mean excludes the largest and smallest five states (trim=0.1 drops 10% from each end). If we want to compute the average murder rate for the country, we need to use a weighted mean or median to account for different populations in the states. Since base `R` doesn’t have a function for weighted median, we need to install a package such as `matrixStats`:

```{r}
## Get the weighted Mean
weighted.mean(state[["Murder.Rate"]], w=state[["Population"]])

## use the `matrixStats` package to get the weighted median
weightedMedian(state[["Murder.Rate"]], w=state[["Population"]])
```

In this case, the weighted mean and median are about the same.

| Key Ideas |
|-----|
| The basic metric for location is the mean, but it can be sensitive to extreme values (outlier). |
| Other metrics (median, trimmed mean) are more robust. |

# Estimates of Variability

Location is just one dimension in summarizing a feature. A second dimension, ***variability***, also referred to as ***dispersion***, measures whether the data values are tightly clustered or spread out. At the heart of statistics lies variability: measuring it, reducing it, distinguishing random from real variability, identifying the various sources of real variability, and making decisions in the presence of it.

| Term | Definition | Synonym |
|-----|-----|-----|
| **Deviations** | The difference between the observed values and the estimate of location. | errors, residuals |
| **Variance** | The sum of squared deviations from the mean divided by $n – 1$ where $n$ is the number of data values. | mean-squared-error (MSE) |
| **Standard Deviation** | The square root of the variance. | l2-norm, Euclidean Norm |
| **Mean Absolute Deviation** | The mean of the absolute value of the deviations from the mean. | l1-norm, Manhattan Norm |
| **Median Absolute Deviation from the Median** | The median of the absolute value of the deviations from the median.||
| **Range** | The difference between the largest and the smallest value in a data set. ||
| **Order Statistics** | Metrics based on the data values sorted from smallest to biggest. | ranks |
| **Percentile** | The value such that $P$ percent of the values take on this value or less and ($100–P$) percent take on this value or more.| quantile |
| **Interquartile Range** | The difference between the 75th percentile and the 25th percentile. | IQR |

## Standard Deviation and Related Estimates

The most widely used estimates of variation are based on the differences, or deviations, between the estimate of location and the observed data. For a set of data {1, 4, 4}, the mean is 3 and the median is 4. The deviations from the mean are the differences: 1 – 3 = –2, 4 – 3 = 1 , 4 – 3 = 1. These deviations tell us how dispersed the data is around the central value.

One way to measure variability is to estimate a typical value for these deviations. Averaging the deviations themselves would not tell us much—the negative deviations offset the positive ones. In fact, the sum of the deviations from the mean is precisely zero. Instead, a simple approach is to take the average of the absolute values of the deviations from the mean. In the preceding example, the absolute value of the deviations is {2 1 1} and their average is (2 + 1 + 1) / 3 = 1.33. This is known as the ***mean absolute deviation*** and is computed with the formula:

$MeanAbsoluteDeviation = \frac{\Sigma^n_{i=1}|x_i - \bar{x}|}{n}$

where $\bar{x}$ is the sample mean.

The best-known estimates for variability are the ***variance*** and the ***standard deviation***, which are based on squared deviations. The variance is an average of the squared deviations, and the standard deviation is the square root of the variance.

$Variance = s^2 = \frac{\Sigma(x - \bar{x})^2}{n-1}$
$Standard Deviation = s = \sqrt{Variance}$

The standard deviation is much easier to interpret than the variance since it is on the same scale as the original data. Still, with its more complicated and less intuitive formula, it might seem peculiar that the standard deviation is preferred in statistics over the mean absolute deviation. It owes its preeminence to statistical theory: mathematically, working with squared values is much more convenient than absolute values, especially for statistical models.

<center> **Degrees of Freedom, and $N$ or $N-1$?**</center>
> In statistics books, there is always some discussion of why we have $n – 1$ in the denominator in the variance formula, instead of $n$, leading into the concept of ***degrees of freedom***. This distinction is not important since n is generally large enough that it won’t make much difference whether you divide by $n$ or $n – 1$. It is based on the premise that you want to make estimates about a population, based on a sample.

> If you use the intuitive denominator of $n$ in the variance formula, you will underestimate the true value of the variance and the standard deviation in the population. This is referred to as a **biased** estimate. However, if you divide by $n – 1$ instead of $n$, the standard deviation becomes an ***unbiased*** estimate.

> To fully explain why using $n$ leads to a biased estimate involves the notion of degrees of freedom, which takes into account the number of constraints in computing an estimate. In this case, there are $n – 1$ degrees of freedom since there is one constraint: the standard deviation depends on calculating the sample mean. For many problems, data scientists do not need to worry about degrees of freedom, but there are cases where the concept is important, like choosing $K$.

Neither the variance, the standard deviation, nor the mean absolute deviation is robust to outliers and extreme values. The variance and standard deviation are especially sensitive to outliers since they are based on the squared deviations.

A robust estimate of variability is the ***median absolute deviation from the median*** or ***MAD***:

$MedianAbsoluteDeviation = Median(| x_1 - m |, | x_2 - m |,  ..., | x_N - m |)$

where $m$ is the median. Like the median, the MAD is not influenced by extreme values. It is also possible to compute a trimmed standard deviation analogous to the trimmed mean.

## Estimates Based on Percentiles

A different approach to estimating dispersion is based on looking at the spread of the sorted data. Statistics based on sorted (ranked) data are referred to as ***order statistics***. The most basic measure is the ***range***: the difference between the largest and smallest number. The minimum and maximum values themselves are useful to know, and helpful in identifying outliers, but the range is extremely sensitive to outliers and not very useful as a general measure of dispersion in the data.

To avoid the sensitivity to outliers, we can look at the range of the data after dropping values from each end. Formally, these types of estimates are based on differences between ***percentiles***. In a data set, the $P$th percentile is a value such that at least $P$ percent of the values take on this value or less and at least ($100 – P$) percent of the values take on this value or more. For example, to find the 80th percentile, sort the data. Then, starting with the smallest value, proceed 80 percent of the way to the largest value. Note that the median is the same thing as the 50th percentile. The percentile is essentially the same as a ***quantile***, with quantiles indexed by fractions (so the .8 quantile is the same as the 80th percentile).

A common measurement of variability is the difference between the 25th percentile and the 75th percentile, called the ***interquartile range*** (or ***IQR***). Here is a simple example: $3,1,5,3,6,7,2,9$. We sort these to get $1,2,3,3,5,6,7,9$. The 25th percentile is at $2.5$, and the 75th percentile is at $6.5$, so the interquartile range is $6.5 – 2.5 = 4$.

Using R’s built-in functions for the standard deviation, interquartile range (IQR), and the median absolution deviation from the median (MAD), we can compute estimates of variability for the state population data:

```{r}
## Get the Standard Deviation for the Population in the State dataset
sd(state[["Population"]])

## Get the Interquartile Range
IQR(state[["Population"]])

## Get the Median Absolute Deviation from the Median
mad(state[["Population"]])
```

The standard deviation is almost twice as large as the MAD (in R, by default, the scale of the MAD is adjusted to be on the same scale as the mean). This is not surprising since the standard deviation is sensitive to outliers.

|Key Ideas|
|-----|
|The variance and standard deviation are the most widespread and routinely reported statistics of variability.|
|Both are sensitive to outliers.|
|More robust metrics include mean and median absolute deviations from the mean and percentiles (quantiles).|

# Exploring the Data Distribution

Each of the estimates we’ve covered sums up the data in a single number to describe the location or variability of the data. It is also useful to explore how the
data is distributed overall.

|Term|Definition|Synonym|
|-----|-----|-----|
|**Boxplot**|A plot introduced by Tukey as a quick way to visualize the distribution of data.|Box and whiskers plot|
|**Frequency Table**| A tally of the count of numeric data values that fall into a set of intervals (bins).||
|**Histogram**|A plot of the frequency table with the bins on the x-axis and the count (or proportion) on the y-axis.||
|**Density Plot** | A smoothed version of the histogram, often based on a ***kernal density estimate***.||

## Percentiles and Boxplots

Percentiles are also valuable to summarize the entire distribution. It is common to report the quartiles (25th, 50th, and 75th percentiles) and the deciles (the 10th, 20th, ..., 90th percentiles). Percentiles are especially valuable to summarize the tails (the outer range) of the distribution.

In R, this would be produced by the `quantile` function:

```{r}
## Get the quantile ranges for murder rate
quantile(state[["Murder.Rate"]], p = c(.05, .25, .5, .75, .95))
```

The median is 4 murders per 100,000 people, although there is quite a bit of variability: the 5th percentile is only 1.6 and the 95th percentile is 6.51.

Boxplots, introduced by Tukey, are based on percentiles and give a quick way to visualize the distribution of data. A boxplot of the population by state produced by R:

```{r}
boxplot(state[["Population"]]/1000000, ylab="Population (millions)")
```

The top and bottom of the box are the 75th and 25th percentiles, respectively. The median is shown by the horizontal line in the box. The dashed lines, referred to as *whiskers*, extend from the top and bottom to indicate the range for the bulk of the data. There are many variations of a boxplot; see, for example, the documentation for the R function `boxplot`. By default, the R function extends the whiskers to the furthest point beyond the box, except that it will not go beyond 1.5 times the IQR (other software may use a different rule). Any data outside of the whiskers is plotted as single points.

## Frequency Table and Histograms

A frequency table of a variable divides up the variable range into equally spaced segments, and tells us how many values fall in each segment. 

```{r}
## Create Breaks for the histogram, into 11 sections
breaks <- seq(from = min(state[["Population"]]), 
              to = max(state[["Population"]]),
              length = 11)

## Create the frequency cuts based on the breaks
pop_freq <- cut(state[["Population"]], 
                breaks = breaks,
                right = TRUE,
                include.lowest = TRUE)

## Create the table
table(pop_freq)
```


A histogram is a way to visualize a frequency table, with bins on the x-axis and data count on the y-axis. To create a histogram corresponding in R, use the hist function with the breaks argument:

```{r}
## Creating a histogram
hist(state[["Population"]],
     breaks = breaks)
```


In general, histograms are plotted such that:

* Empty bins are included in the graph.
* Bins are equal width.
* Number of bins (or, equivalently, bin size) is up to the user.
* Bars are contiguous—no empty space shows between bars, unless there is an empty bin.

## Density Estimates

Related to the histogram is a density plot, which shows the distribution of data values as a continuous line. A density plot can be thought of as a smoothed histogram, although it is typically computed directly from the data through a ***kernal density estimate***. In R, you can compute a density estimate using the density function:

```{r}
## Create a hist and a density plot
hist(state[["Murder.Rate"]], freq = FALSE)
lines(density(state[["Murder.Rate"]]), 
              ## Line Weight
              lwd = 3, 
              ## Line Color
              col = "blue")
```
A key distinction from the histogram plotted is the scale of the y- axis: a density plot corresponds to plotting the histogram as a proportion rather than counts (you specify this in R using the argument freq=FALSE).

|Key Ideas|
|-----|
|A frequency histogram plots frequency counts on the y-axis and variable values on the x-axis; it gives a sense of the distribution of the data at a glance.|
|A frequency table is a tabular version of the frequency counts found in a histogram.|
|A boxplot—with the top and bottom of the box at the 75th and 25th percentiles, respectively—also gives a quick sense of the distribution of the data; it is often used in side-by-side displays to compare distributions.|
|A density plot is a smoothed version of a histogram; it requires a function to estimate a plot based on the data (multiple estimates are possible, of course).|

# Exploring Binary and Categorical Data

For categorical data, simple proportions or percentages tell the story of the data.

| Term | Definition |
| ----- | -----|
| **Mode** | The most commonly occurring category or value in a data set. |
| **Expected Value** | When the categories can be associated with a numeric value, this gives an average value based on a category’s probability of occurrence. | 
| **Bar Charts** | The frequency or proportion for each category plotted as bars. |
| **Pie Charts** | The frequency or proportion for each category plotted as wedges in a pie. |
Table: **KEY TERMS FOR EXPLORING CATEGORICAL DATA**

Getting a summary of a binary variable or a categorical variable with a few categories is a fairly easy matter: we just figure out the proportion of 1s, or of the important categories. 

Bar charts are a common visual tool for displaying a single categorical variable, often seen in the popular press. Categories are listed on the x-axis, and frequencies or proportions on the y-axis.

```{r}
barplot(as.matrix(dfw)/6, cex.axis = 5)
```

Note that a bar chart resembles a histogram; in a bar chart the x-axis represents different categories of a factor variable, while in a histogram the x-axis represents values of a single variable on a numeric scale. In a histogram, the bars are typically shown touching each other, with gaps indicating values that did not occur in the data. In a bar chart, the bars are shown separate from one another.

Pie charts are an alternative to bar charts, although statisticians and data visualization experts generally eschew pie charts as less visually informative.

## Mode 
The mode is the value—or values in case of a tie—that appears most often in the data. The mode is a simple summary statistic for categorical data, and it is generally not used for numeric data.

## Expected Value
A special type of categorical data is data in which the categories represent or can be mapped to discrete values on the same scale. This data can be summed up, for financial purposes, in a single “expected value,” which is a form of weighted mean in which the weights are probabilities.

The expected value is calculated as follows:

1) Multiply each outcome by its probability of occuring.
2) sum these values.

The expected value is really a form of weighted mean: it adds the ideas of future expectations and probability weights, often based on subjective judgment. Expected value is a fundamental concept in business valuation and capital budgeting.

|Key Ideas|
|-----|
|Categorical data is typically summed up in proportions, and can be visualized in a bar chart.|
|Categories might represent distinct things (apples and oranges, male and female), levels of a factor variable (low, medium, and high), or numeric data that has been binned.|
|Expected value is the sum of values times their probability of occurrence, often used to sum up factor variable levels.|

# Correlation
Exploratory data analysis in many modeling projects (whether in data science or in research) involves examining correlation among predictors, and between predictors and a target variable. Variables X and Y (each with measured data) are said to be positively correlated if high values of X go with high values of Y, and low values of X go with low values of Y. If high values of X go with low values of Y, and vice versa, the variables are negatively correlated.

| Term | Definition |
|-----|-----|
| **Correlation Coefficient** | A metric that measures the extent to which numeric variables are associated with one another (ranges from –1 to +1). |
| **Correlation Matrix** | A table where the variables are shown on both rows and columns, and the cell values are the correlations between the variables. |
| **Scatterplot** | A plot in which the x-axis is the value of one variable, and the y-axis the value of another. |

More useful is a standardized variant: the ***correlation coefficient***, which gives an estimate of the correlation between two variables that always lies on the same scale. To compute ***Pearson’s correlation coefficient***, we multiply deviations from the mean for variable 1 times those for variable 2, and divide by the product of the standard deviations:

$r = \frac{\Sigma^N_{i=1}(x_i-\bar{x})(y_i-\bar{y})}{(N-1)s_xs_y}$

The correlation coefficient always lies between +1 (perfect positive correlation) and –1 (perfect negative correlation); 0 indicates no correlation.

Variables can have an association that is not linear, in which case the correlation coefficient may not be a useful metric. 

In R, we can easily create a ***correlation matrix*** using the package corrplot:

```{r}
etfs <- sp500_px[row.names(sp500_px) > "2017-07-01",
                 sp500_sym[sp500_sym$sector=="etf", 'symbol']]

corrplot(cor(etfs), method = "ellipse")
```

## Other Correlation Estimates

Statisticians have long ago proposed other types of correlation coefficients, such as Spearman’s rho or Kendall’s tau. These are correlation coefficients based on the rank of the data. Since they work with ranks rather than values, these estimates are robust to outliers and can handle certain types of nonlinearities. However, data scientists can generally stick to Pearson’s correlation coefficient, and its robust alternatives, for exploratory analysis. The appeal of rank- based estimates is mostly for smaller data sets and specific hypothesis tests.

## Scatterplots
The standard way to visualize the relationship between two measured data variables is with a scatterplot. The x-axis represents one variable, the y-axis another, and each point on the graph is a record.

```{r}
plot(sp500_px$T, sp500_px$VZ, xlab = "T", ylab = "VZ")
```

|Key Ideas|
|-----|
|The correlation coefficient measures the extent to which two variables are associated with one another.|
|When high values of v1 go with high values of v2, v1 and v2 are positively associated.|
|When high values of v1 are associated with low values of v2, v1 and v2 are negatively associated.|
|The correlation coefficient is a standardized metric so that it always ranges from –1 (perfect negative correlation) to +1 (perfect positive correlation).|
|A correlation coefficient of 0 indicates no correlation, but be aware that random arrangements of data will produce both positive and negative values for the correlation coefficient just by chance.|

# Exploring Two or More Variables

Familiar estimators like mean and variance look at variables one at a time (univariate analysis). Correlation analysis is an important method that compares two variables (bivariate analysis). In this section we look at additional estimates and plots, and at more than two variables (multivariate analysis).

|Term|Definition|
|----|-----|
| **Contingency Tables** | A tally of counts between two or more categorical variables.|
| **Hexagonal Binning** | A plot of two numeric variables with the records binned into hexagons. |
| **Contour Plots** | A plot showing the density of two numeric variables like a topographical map. |
| **Violin Plots** | Similar to a boxplot but showing the density estimate. |
Table: **KEY TERMS FOR EXPLORING TWO OR MORE VARIABLES**

Like univariate analysis, bivariate analysis involves both computing summary statistics and producing visual displays. The appropriate type of bivariate or multivariate analysis depends on the nature of the data: numeric versus categorical.

## Hexagonal Binning and Contours (Plotting Numeric versus Numeric Data)

Scatterplots are fine when there is a relatively small number of data values. For data sets with hundreds of thousands or millions of records, a scatterplot will be too dense, so we need a different way to visualize the relationship.

```{r}
kc_tax0 <- subset(kc_tax, TaxAssessedValue < 750000 & 
                         SqFtTotLiving > 100 & 
                         SqFtTotLiving < 3500)

nrow(kc_tax0)
```

Rather than plotting points, which would appear as a monolithic dark cloud, we grouped the records into hexagonal bins and plotted the hexagons with a color indicating the number of records in that bin. 

```{r}
ggplot(data = kc_tax0, mapping = aes(x = SqFtTotLiving, y = TaxAssessedValue)) +
  stat_binhex(colour = "white") +
  theme_bw() +
  scale_fill_gradient(low = "white", high = "black") + 
  labs(x = "Finished Square Feet", y = "Tax Assessed Value")
```

The contours are essentially a topographical map to two variables; each contour band represents a specific density of points, increasing as one nears a “peak.” 

```{r}
ggplot(kc_tax0, mapping = aes(x = SqFtTotLiving, y = TaxAssessedValue)) +
  theme_bw() + 
  geom_point(alpha = 0.1) + 
  geom_density2d(colour = "white") + 
  labs(x = "Finished Sqaure Feet", y = "Tax Assessed Value")
```

Other types of charts are used to show the relationship between two numeric variables, including heat maps. Heat maps, hexagonal binning, and contour plots all give a visual representation of a two-dimensional density. In this way, they are natural analogs to histograms and density plots.

## Two Categorical Variables

A useful way to summarize two categorical variables is a contingency table—a table of counts by category. 

```{r}
x_tab <- CrossTable(lc_loans$grade, lc_loans$status,
                    prop.c = FALSE, prop.chisq = FALSE, prop.t = FALSE)

x_tab
```

## Categorical and Numeric Data

Boxplots are a simple way to visually compare the distributions of a numeric variable grouped according to a categorical variable.

```{r}
boxplot(pct_atc_delay + pct_weather_delay + pct_carrier_delay ~ airline, data = airline_stats, ylim=c(0, 50))
```

A ***violin plot*** is an enhancement to the boxplot and plots the density estimate with the density on the y-axis. The density is mirrored and flipped over and the resulting shape is filled in, creating an image resembling a violin. The advantage of a violin plot is that it can show nuances in the distribution that aren’t perceptible in a boxplot. On the other hand, the boxplot more clearly shows the outliers in the data.

```{r}
ggplot(data = airline_stats, mapping = aes(airline, pct_carrier_delay)) + 
  ylim(0, 50) +
  geom_violin() + 
  labs(x = "", y = "Daily % of Delayed Flights")
```

## Visualizing Multiple Variables

The types of charts used to compare two variables—scatterplots, hexagonal binning, and boxplots—are readily extended to more variables through the notion of ***conditioning***. 

```{r}
ggplot(subset(kc_tax0, ZipCode %in% c(98188, 98105, 98108, 98126)), aes(x=SqFtTotLiving, y=TaxAssessedValue)) +
  stat_binhex(colour = "white") + 
  theme_bw() + 
  scale_fill_gradient(low = "white", high = "blue") +
  labs(x="Finished Square Feet", y = "Tax Assessed Value") +
  facet_wrap("ZipCode")
```

Conditioning variables are also integral to business intelligence platforms such as Tableau and Spotfire. With the advent of vast computing power, modern visualization platforms have moved well beyond the humble beginnings of exploratory data analysis. However, key concepts and tools developed over the years still form a foundation for these systems.

| Key Ideas|
|-----|
| Hexagonal binning and contour plots are useful tools that permit graphical examination of two numeric variables at a time, without being overwhelmed by huge amounts of data. |
| Contingency tables are the standard tool for looking at the counts of two categorical variables. |
| Boxplots and violin plots allow you to plot a numeric variable against a categorical variable. |